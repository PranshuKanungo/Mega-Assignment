Scenario Based questions:

Que : 1 Will the reducer work or not if you use “Limit 1” in any HiveQL query?

Ans: The Reducers are used when there is any aggregation or any other operation to be executed so whether the reducer will work or not will completely 
depends on the query we have written.For example: If your query is - select * from table;
Here, reducers will not work because there is no aggregation function. And if your query is something which includes aggregation along with group by or order by 
then it will use the reducers. i.e if you use your map-reduce as your execution engine then the reducer will come into the pitcure.
For Ex. select * from table order by column;
It uses reducers or whenever there is an aggregation.

Que:2 Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration.
Then, what will happen if we have multiple clients trying to access Hive at the same time? 

Ans: The default metastore configuration allows only one Hive session to be opened at a time for accessing the metastore. Therefore, if multiple clients try to access
the metastore at the same time, they will get an error. One has to use a standalone metastore, i.e. Local or remote metastore configuration in Apache Hive for
allowing access to multiple clients concurrently.

Que: 3 Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT,
month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ; Now, after inserting 50,000 records in this table, I want to know the total revenue generated 
for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?

Ans:We can solve this problem of query latency by partitioning the table according to each month. So, for each month we will be scanning only the partitioned data 
instead of whole data sets. As we know, we can’t partition an existing non-partitioned table directly. So, we will be taking following steps to solve the very problem: 

1.Create a partitioned table, say partitioned_transaction:

CREATE TABLE partitioned_transaction (cust_id INT, amount FLOAT, country STRING) PARTITIONED BY (month STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ; 

2. Enable dynamic partitioning in Hive:

SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;

3. Transfer the data from the non – partitioned table into the newly created partitioned table:

INSERT OVERWRITE TABLE partitioned_transaction PARTITION (month) SELECT cust_id, amount, country, month FROM transaction_details;
Now, we can perform the query using each partition and therefore, decrease the query time.

Que: 4 How can you add a new partition for the month December in the above partitioned table?

Ans: For adding a new partition in the above table partitioned_transaction, we will issue the command give below:
ALTER TABLE partitioned_transaction ADD PARTITION (month=’Dec’) LOCATION  ‘/partitioned_transaction’;

Que: 5 I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires
at least one static partition column. How will you remove this error?

Ans: To remove this error one has to execute following commands:
SET hive.exec.dynamic.partition = true;
SET hive.exec.dynamic.partition.mode = nonstrict;
* By default, hive.exec.dynamic.partition configuration property is set to False in case you are using Hive whose version is prior to 0.9.0. 
* hive.exec.dynamic.partition.mode is set to strict by default. Only in non–strict mode Hive allows all partitions to be dynamic.

Que: 6 Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries:
id first_name last_name email gender ip_address How will you consume this CSV file into the Hive warehouse using built-in SerDe?

Ans: SerDe stands for serializer/deserializer. A SerDe allows us to convert the unstructured bytes into a record that we can process using Hive. SerDes are implemented
using Java. Hive comes with several built-in SerDes and many other third-party SerDes are also available. Hive provides a specific SerDe for working with CSV files.
We can use this SerDe for the sample.csv by issuing following commands:

CREATE EXTERNAL TABLE sample
(id int,
first_name string, 
last_name string,
email string,
gender string,
ip_address string) 
ROW FORMAT SERDE ‘org.apache.hadoop.hive.serde2.OpenCSVSerde’ 
STORED AS TEXTFILE LOCATION ‘/temp’;

// Now, we can perform any query on the table ‘sample’:
SELECT first_name FROM sample WHERE gender = ‘male’;


Que:7 Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. 
The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files. So, how will you solve
this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?

Ans: We can use the SequenceFile format which will group these small files together to form a single sequence file. The steps that will be followed in doing so are as follows:
* Create a temporary table:
CREATE TABLE temp_table (id INT, name STRING, e-mail STRING, country STRING)
ROW FORMAT FIELDS DELIMITED TERMINATED BY ‘,’ STORED AS TEXTFILE;

* Load the data into temp_table:
LOAD DATA INPATH ‘/input’ INTO TABLE temp_table;

* Create a table that will store data in SequenceFile format:
CREATE TABLE sample_seqfile (id INT, name STRING, e-mail STRING, country STRING)
ROW FORMAT FIELDS DELIMITED TERMINATED BY ‘,’ STORED AS SEQUENCEFILE;

* Transfer the data from the temporary table into the sample_seqfile table:
INSERT OVERWRITE TABLE sample SELECT * FROM temp_table;

Hence, a single SequenceFile is generated which contains the data present in all of the input files and therefore, the problem of having lots of small files is
finally eliminated.

Que: 8 LOAD DATA LOCAL INPATH ‘Home/country/state/’
OVERWRITE INTO TABLE address; 
The following statement failed to execute. What can be the cause?

Ans: The local inpath should contain a file and not a directory. The $env:HOME is a valid variable available in the hive environment.

Que: 9 Is it possible to add 100 nodes when we already have 100 nodes in Hive? If yes, how?

Ans: Yes, we can add the nodes by following the below steps:

Step 1: Take a new system; create a new username and password
Step 2: Install SSH and with the master node setup SSH connections
Step 3: Add ssh public_rsa id key to the authorized keys file
Step 4: Add the new DataNode hostname, IP address, and other details in /etc/hosts slaves file:

192.168.1.102 slave3.in slave3
Step 5: Start the DataNode on a new node
Step 6: Login to the new node like suhadoop or:

ssh -X hadoop@192.168.1.103
Step 7: Start HDFS of the newly added slave node by using the following command:

./bin/hadoop-daemon.sh start data node
Step 8: Check the output of the jps command on the new node

Hive Practical questions:

Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT)

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

Ans: 

Create table if not exits customers(
id int,
name VARCHAR(30),
age int,
address Varchar(50),
salary int)
row format delimited
fields terminated by ',';

load data local inpath 'file:///tmp/hive_class/customers.csv' into table customers;

Create table if not exists order(
o_id int,
date date,
customer_id int,
amount int)
row format delimited,
fields terminated by ',';

load data local inpath 'file:///tmp/hive_class/order.csv' into table order;

create table if not exists buck_customers(
id int,
name VARCHAR(30),
age int,
address Varchar(50),
salary int)
clustered by (id),
sorted  by (id),
into 2 Buckets;

#load into buck_customers from customers table
insert overwrite table buck_customers select * from customers;

create table buck_order
     (
     o_id int,
     date date,
     customer_id int,
     amount int )
     clustered by (o_id)
     sorted by (o_id)
     into 2 buckets;
    
#load into buck_order from order table
insert overwrite table buck_order select * from order;


* Reduce-side Join

SET hive.auto.convert.join = false;
SELECT * from buck_customers c INNER JOIN buck_orders o on c.id = o.customer_id;  //no. of mappers: 2 ; no. of reducers : 1 

1	Ajay	28	Indore	100	1	2021-02-25	1	30
2	Amit	25	Chhindwara	200	2	2022-04-29	2	50
3	Sumit	23	Delhi	300	3	2022-05-19	3	70
5	Ravi	24	Pune	300	4	2022-06-10	5	40
6	Hariom	22	Agra	700	5	2022-07-15	6	80

* Map-Side Join

SET hive.auto.convert.join = true;
SELECT * FROM buck_customers c INNER JOIN buck_orders o on c.id = o.customer_id;

6	Hariom	22	Agra	700	5	2022-07-15	6	80
2	Amit	25	Chhindwara	200	2	2022-04-29	2	50
5	Ravi	24	Pune	300	4	2022-06-10	5	40
3	Sumit	23	Delhi	300	3	2022-05-19	3	70
1	Ajay	28	Indore	100	1	2021-02-25	1	30


* Bucket Map Join 

SET hive.optimize.bucketmapjoin = true;
SET hive.auto.convert.join = true;
SELECT * FROM buck_customers c INNER JOIN buck_orders o ON c.id = o.customer_id

6	Hariom	22	Agra	700	5	2022-07-15	6	80
2	Amit	25	Chhindwara	200	2	2022-04-29	2	50
5	Ravi	24	Pune	300	4	2022-06-10	5	40
3	Sumit	23	Delhi	300	3	2022-05-19	3	70
1	Ajay	28	Indore	100	1	2021-02-25	1	30

* Sorted MERGE bucket Map Join

set hive.enforce.sortmergebucketmapjoin = false;
set hive.auto.convert.join = true;
set hive.optimize.bucketmapjoin = true;
set hive.optimize.bucketmapjoin.sortedmerge = true;
set hive.auto.convert.join = false;

Select * from buck_cutomers c INNER JOIN buck_orders o ON c.id = o.customer_id;

6	Hariom	22	Agra	700	5	2022-07-15	6	80
2	Amit	25	Chhindwara	200	2	2022-04-29	2	50
5	Ravi	24	Pune	300	4	2022-06-10	5	40
3	Sumit	23	Delhi	300	3	2022-05-19	3	70
1	Ajay	28	Indore	100	1	2021-02-25	1	30


BUILD A DATA PIPELINE WITH HIVE

Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset 
2. try to place a data into table location
3. Perform a select operation . 
4. Fetch the result of the select operation in your local as a csv file . 
5. Perform group by operation . 
7. Perform filter operation at least 5 kinds of filter examples . 
8. show and example of regex operation
9. alter table operation 
10 . drop table operation
12 . order by operation . 
13 . where clause operations you have to perform . 
14 . sorting operation you have to perform . 
15 . distinct operation you have to perform . 
16 . like an operation you have to perform . 
17 . union operation you have to perform . 
18 . table view operation you have to perform . 






hive operation with python

Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.fetch rows to python itself into a list of tuples and mimic the join or filter operations
